import cv2
import numpy as np

def detect_stop_sign_simple(frame, debug=False):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∑–Ω–∞–∫–∞ STOP:
    - –ü–æ –∫—Ä–∞—Å–Ω–æ–º—É —Ü–≤–µ—Ç—É –≤ HSV
    - –ü–æ —Ä–∞–∑–º–µ—Ä—É –∫–æ–Ω—Ç—É—Ä–∞
    - –ü–æ —Ñ–æ—Ä–º–µ (–º–Ω–æ–≥–æ—É–≥–æ–ª—å–Ω–∏–∫, –±–ª–∏–∑–∫–∏–π –∫ –≤–æ—Å—å–º–∏—É–≥–æ–ª—å–Ω–∏–∫—É)
    """

    # –ü–æ—Ä–æ–≥–∏ –∫—Ä–∞—Å–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞ (–º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –ø–æ–¥–∫—Ä—É—Ç–∏—Ç—å –ø–æ–¥ —Å–≤–æ–∏ –≤–∏–¥–µ–æ)
    lower_red1 = np.array([0, 100, 100])
    upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([12, 127, 203])
    upper_red2 = np.array([179, 255, 238])

    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

    # –ú–∞—Å–∫–∞ –∫—Ä–∞—Å–Ω–æ–≥–æ
    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
    red_mask = mask2

    # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
    kernel = np.ones((5, 5), np.uint8)
    red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, kernel)
    red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)

    contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    h, w = frame.shape[:2]
    frame_area = h * w

    for contour in contours:
        area = cv2.contourArea(contour)
        if area < frame_area * 0.001 or area > frame_area * 0.3:
            continue  # —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –æ–≥—Ä–æ–º–Ω—ã–µ

        # –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –º–Ω–æ–≥–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–º
        perimeter = cv2.arcLength(contour, True)
        if perimeter == 0:
            continue

        approx = cv2.approxPolyDP(contour, 0.03 * perimeter, True)
        vertices = len(approx)

        # STOP ‚Äì –æ–±—ã—á–Ω–æ 8-—É–≥–æ–ª—å–Ω–∏–∫, –Ω–æ –¥–∞—ë–º –¥–æ–ø—É—Å–∫
        if 6 <= vertices <= 10:
            x, y, w_box, h_box = cv2.boundingRect(contour)
            aspect_ratio = w_box / float(h_box)
            if 0.7 < aspect_ratio < 1.3:  # –ø–æ—á—Ç–∏ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π
                if debug:
                    cv2.drawContours(frame, [approx], -1, (0, 255, 0), 2)
                    cv2.putText(frame, "STOP?", (x, y - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                    cv2.imshow("stop_debug", frame)
                    cv2.waitKey(1)
                return True

    return False

import cv2
import numpy as np
from collections import deque

def detect_line_direction(opt_frame, debug=False):
    """
    –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º:
    - –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
    - 1D —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
    - –ù–∞–¥—ë–∂–Ω—ã–π –ø–æ–∏—Å–∫ –º–∞–∫—Å–∏–º—É–º–∞
    """

    h, w, _ = opt_frame.shape
    center_x = w // 2

    # 1. ROI –Ω–∏–∂–Ω—è—è —á–∞—Å—Ç—å –∫–∞–¥—Ä–∞
    roi_y_start = int(h * 0.45)
    roi = opt_frame[roi_y_start:]
    roi_h, roi_w = roi.shape[:2]

    # 2. –ì—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

    # 3. –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è
    _, binary = cv2.threshold(
        gray, 0, 255,
        cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    # 4. –£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–æ–≤
    kernel = np.ones((5, 5), np.uint8)
    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)

    # 5. –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è (sum –ø–æ y)
    col_sum = binary.sum(axis=0).astype(np.float32)

    # –ï—Å–ª–∏ –ª–∏–Ω–∏–∏ –Ω–µ—Ç
    if col_sum.max() < 50:  # –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥
        return "none", 0.0

    # 6. 1D —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
    col_sum = cv2.GaussianBlur(col_sum.reshape(-1,1), (9,1), 0).flatten()

    # 7. –ü–æ–∏—Å–∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –º–∞–∫—Å–∏–º—É–º–∞
    line_x = int(np.argmax(col_sum))

    # 8. –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π offset
    offset_px = line_x - center_x
    offset_norm = offset_px / center_x

    # 9. –ü–æ—Ä–æ–≥ –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    threshold = 0.05

    if offset_norm < -threshold:
        direction = "left"
    elif offset_norm > threshold:
        direction = "right"
    else:
        direction = "none"

    # ===== Debug visualization =====
    if debug:
        dbg = opt_frame.copy()
        cv2.line(dbg, (center_x, 0), (center_x, h), (0, 255, 0), 2)
        cv2.circle(dbg, (line_x, h - 10), 8, (0, 0, 255), -1)
        cv2.putText(dbg, f"{direction} off={offset_norm:.2f}",
                    (10, 40), cv2.FONT_HERSHEY_SIMPLEX,
                    1, (255,255,255), 2)
        cv2.imshow("line_debug", dbg)
        cv2.imshow("binary", binary)
        cv2.imshow("projection", col_sum.astype(np.uint8))
        cv2.waitKey(1)

    return direction, float(offset_norm)




def process_dual_videos(main_video_path, opt_video_path, output_file="generated_check.txt",
                        smooth_window=5):
    main_cap = cv2.VideoCapture(main_video_path)
    opt_cap = cv2.VideoCapture(opt_video_path)
    
    total_frames = int(main_cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_count = 0
    stop_detected_frame = 0
    stop_found = False

    markup_data = []

    navigation_stats = {
        "left": 0,
        "right": 0, 
        "none": 0,
        "total_processed": 0
    }

    # –æ—á–µ—Ä–µ–¥—å –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è
    offset_history = deque(maxlen=smooth_window)

    # === –ù–æ–≤–æ–µ: —Å—á—ë—Ç—á–∏–∫ –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö –∫–∞–¥—Ä–æ–≤ —Å–æ –∑–Ω–∞–∫–æ–º STOP ===
    stop_streak = 0
    stop_threshold_frames = 5  # —Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ä—è–¥ –∫–∞–¥—Ä–æ–≤ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã —Å—á–∏—Ç–∞—Ç—å –∑–Ω–∞–∫ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º

    print(f"–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤ –≤ –≤–∏–¥–µ–æ: {total_frames}")
    print("–ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–≤—É—Ö –∫–∞–º–µ—Ä...")
    print("–ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
    
    while True:
        ret_main, main_frame = main_cap.read()
        ret_opt, opt_frame = opt_cap.read()
        
        if not ret_main or not ret_opt:
            break
        
        frame_count += 1
        
        # ---- 1. –°—Ç–æ–ø-–∑–Ω–∞–∫ –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞–º–µ—Ä–µ ----
        is_stop_raw = False
        if frame_count / total_frames > 0.6:  # –ø–æ—Å–ª–µ 80% –Ω–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫
            is_stop_raw = detect_stop_sign_simple(main_frame)

        # –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏–µ –∫–∞–¥—Ä—ã —Å –¥–µ—Ç–µ–∫—Ç–æ–º
        if is_stop_raw:
            stop_streak += 1
        else:
            stop_streak = 0

        # —Å—á–∏—Ç–∞–µ–º –∑–Ω–∞–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã–º, –µ—Å–ª–∏ –ø–æ–¥—Ä—è–¥ >= N –∫–∞–¥—Ä–æ–≤ –≤–∏–¥–∏–º STOP
        is_stop = stop_streak >= stop_threshold_frames

        # –æ–¥–∏–Ω —Ä–∞–∑ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –∫–∞–¥—Ä –ø–µ—Ä–≤–æ–≥–æ —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
        if is_stop and not stop_found:
            stop_found = True
            stop_detected_frame = frame_count
        
        # ---- 2. –õ–∏–Ω–∏—è –Ω–∞ –æ–ø—Ç–∏—á–µ—Å–∫–æ–π –∫–∞–º–µ—Ä–µ ----
        direction_raw, offset_norm_raw = detect_line_direction(opt_frame)
        offset_history.append(offset_norm_raw)

        # —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ
        if len(offset_history) > 0:
            offset_norm_smooth = float(sum(offset_history) / len(offset_history))
        else:
            offset_norm_smooth = offset_norm_raw

        # –ø–µ—Ä–µ—Å—á—ë—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–º—É
        threshold = 0.055
        if offset_norm_smooth < -threshold:
            direction = "left"
        elif offset_norm_smooth > threshold:
            direction = "right"
        else:
            direction = "none"

        navigation_stats[direction] += 1
        navigation_stats["total_processed"] += 1
        
        # ---- 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ç–∫—É ----
        # —Ñ–æ—Ä–º–∞—Ç: –∫–∞–¥—Ä, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ (—Å–≥–ª–∞–∂–µ–Ω–Ω–æ–µ)
        markup_data.append(f"{frame_count} {direction} {offset_norm_smooth:.4f}")
        
        # ---- 4. –û—Ç—Ä–∏—Å–æ–≤–∫–∞ ----
        main_display = main_frame.copy()
        opt_display = opt_frame.copy()
        
        cv2.putText(main_display, f"Main Cam - Frame: {frame_count}/{total_frames}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        if is_stop:
            cv2.putText(main_display, "STOP SIGN DETECTED!", (10, 60), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        else:
            status = "Searching..." if frame_count / total_frames > 0.8 else "Waiting 80%..."
            cv2.putText(main_display, f"Stop Sign: {status}", (10, 60), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
        
        cv2.putText(opt_display, f"Optical Cam - Dir: {direction}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        cv2.putText(opt_display, f"Offset: {offset_norm_smooth:.2f}", (10, 55), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
        cv2.putText(opt_display, f"Progress: {frame_count/total_frames*100:.1f}%", (10, 80), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
        
        if main_display.shape != opt_display.shape:
            opt_display = cv2.resize(opt_display, (main_display.shape[1], main_display.shape[0]))
        
        combined = np.vstack([main_display, opt_display])
        cv2.imshow('Dual Camera Processing', combined)
        
        # –≤—ã—Ö–æ–¥–∏–º –ª–∏–±–æ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–º—É STOP, –ª–∏–±–æ –ø–æ 'q'
        if stop_found or cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    main_cap.release()
    opt_cap.release()
    cv2.destroyAllWindows()
    
    save_markup_to_file(markup_data, output_file, stop_found, stop_detected_frame)
    
    return stop_found, stop_detected_frame, frame_count, total_frames, navigation_stats, markup_data


def save_markup_to_file(markup_data, filename, stop_found, stop_frame):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ç–∫—É –≤ —Ñ–∞–π–ª –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ, —á—Ç–æ –∏ check.txt
    """
    with open(filename, 'w') as f:
        for line in markup_data:
            f.write(line + '\n')
    
    print(f"\n‚úÖ –†–∞–∑–º–µ—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {filename}")
    print(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(markup_data)}")
    if stop_found:
        print(f"üõë –ó–Ω–∞–∫ —Å—Ç–æ–ø –æ–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–∞ –∫–∞–¥—Ä–µ: {stop_frame}")

def video_process(main_img, opt_img):
    """
    –ê–Ω–∞–ª–æ–≥ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ eval.py
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        direction: "left"/"right"/"none"
        is_stop: True/False
    """
    direction, _ = detect_line_direction(opt_img)
    is_stop = detect_stop_sign_simple(main_img)
    return direction, is_stop


# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥
if __name__ == "__main__":
    main_video_path = "main/video/main--effiroom.ru.mp4"
    opt_video_path = "main/video/opt--effiroom.ru.mp4"
    output_markup_file = "generated_check.txt"
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±–∞ –≤–∏–¥–µ–æ
    stop_found, stop_frame, processed_frames, total_frames, nav_stats, markup_data = process_dual_videos(
        main_video_path, opt_video_path, output_markup_file
    )
    
    if stop_found:
        frames_remaining = total_frames - stop_frame
        progress_percent = (stop_frame / total_frames) * 100

        print(f" –ö–∞–¥—Ä –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {stop_frame}")
        print(f" –ü—Ä–æ–π–¥–µ–Ω–æ –∫–∞–¥—Ä–æ–≤: {stop_frame}/{total_frames} ({progress_percent:.1f}%)")
    else:
        print(f" –ó–Ω–∞–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
        print(f" –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–¥—Ä–æ–≤: {processed_frames}/{total_frames}")